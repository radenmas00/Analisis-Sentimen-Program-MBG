# -*- coding: utf-8 -*-
"""Final LSTM MBG YT save model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10sMXLzZj-ppb14vgmyTiVZ2m9KI8sGjP

# **LIBRARY**
"""

# Install library untuk mengolah emoji
!pip install emoji

# Install library untuk stemming Bahasa Indonesia
!pip install Sastrawi

# Install library untuk deep learning (akan dipakai untuk LSTM)
!pip install tensorflow

# Install library machine learning (scikit-learn), untuk preprocessing & evaluasi
!pip install tensorflow scikit-learn

# Import library untuk manipulasi data
import pandas as pd
import numpy as np

# Import library untuk visualisasi data
import matplotlib.pyplot as plt
import seaborn as sns

# Import library untuk pembersihan teks
import re
import string
import emoji
import nltk
from bs4 import BeautifulSoup
from textblob import TextBlob
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Import library deep learning (TensorFlow + Keras)
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from keras.layers import LSTM, Dense, SimpleRNN, Embedding, Flatten, Dropout
from keras.activations import softmax
from tensorflow.keras.layers import GlobalMaxPooling1D, ReLU
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import regularizers

# Import library untuk evaluasi model dan split data
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils import class_weight

# Import untuk analisis frekuensi
from collections import Counter

# Download resource NLTK yang dibutuhkan
nltk.download('wordnet')
nltk.download('punkt')

# Nonaktifkan warning
import warnings
warnings.filterwarnings('ignore')

# Import modul untuk mengakses Google Drive dari Colab
from google.colab import drive
drive.mount('/content/drive')

"""# **PREPROCESSING**"""

# Membuat Teks Ditampilkan Full
pd.set_option('display.max_colwidth', None)
# Membaca file CSV dari Google Drive ke DataFrame pandas
df = pd.read_csv('/content/drive/MyDrive/Skripsi_Dimas Aryo Wibowo_21.11.3972/Dataset/Youtube/yt_mbg.csv')

# Menampilkan 5 baris pertama DataFrame untuk cek data
df.head()

# Hitung jumlah baris duplikat
jumlah_duplikat = df.duplicated().sum()

# Tampilkan jumlah duplikat
print(f"Jumlah data duplikat: {jumlah_duplikat} baris")

# Hapus baris duplikat dan reset index
df = df.drop_duplicates().reset_index(drop=True)

# Tampilkan jumlah data setelah penghapusan duplikat
print(f"Jumlah data setelah hapus duplikat: {len(df)} baris")

# Hapus baris yang memiliki nilai kosong di kolom apa pun
df_cleaned = df.dropna()

# Cek kembali jumlah data setelah penghapusan
df_cleaned_info = df_cleaned.info()

df_cleaned_info

"""Cleaning"""

def clean_tweet(text):
    text = re.sub(r'rt\s@\w+:', '', text)  # Hapus retweet tag "RT @username:"
    text = re.sub(r'@\w+', '', text)  # Hapus username mentions
    text = re.sub(r'#\w+', '', text)  # Hapus hashtags
    text = re.sub(r'http\S+', '', text)  # Hapus URL
    text = re.sub(r'[^\x00-\x7F]+', '', text)  # Hapus emoji dan karakter non-ASCII
    text = re.sub(r'[^\w\s]', '', text)  # Hapus tanda baca
    text = re.sub(r'\s+', ' ', text).strip()  # Hapus spasi berlebihan
    return text

# Terapkan fungsi ke kolom full_text
df["clean_text"] = df["Comment"].apply(clean_tweet)

# Menampilkan hasil
df.head(10)

"""Case Folding"""

# Fungsi case folding
def case_folding(text):
    return text.lower()  # Ubah teks menjadi huruf kecil

# Terapkan case folding ke kolom full_text
df["lower_text"] = df["clean_text"].apply(case_folding)
df.head(10)

"""Normalized"""

# Kamus kata slang dan singkatan ke bentuk baku
extended_normalization_dict = {
    'yg': 'yang', 'n': 'dan', 'bkn': 'bukan', 'dgn': 'dengan', 'dg': 'dengan', 'dr': 'dari',
    'aja': 'saja', 'gk': 'tidak', 'ga': 'tidak', 'tdk': 'tidak', 'g': 'tidak', 'nggak': 'tidak',
    'gak': 'tidak', 'kagak': 'tidak', 'kaga': 'tidak', 'abis': 'habis', 'sebenernya': 'sebenarnya',
    'krn': 'karena', 'sb': 'sebab', 'sbnr': 'sebenarnya', 'bnr': 'benar', 'bgt': 'banget',
    'kl': 'kalau', 'klo': 'kalau', 'kalo': 'kalau', 'jd': 'jadi', 'utk': 'untuk', 'unt': 'unit',
    'dlm': 'dalam', 'tp': 'tapi', 'trs': 'terus', 'trus': 'terus', 'udh': 'sudah', 'udah': 'sudah',
    'sdh': 'sudah', 'blm': 'belum', 'lg': 'lagi', 'sm': 'sama', 'ma': 'sama', 'pd': 'pada',
    'tmn': 'teman', 'teman2': 'teman-teman', 'an': 'akan', 'bbrp': 'beberapa', 'org': 'orang',
    'orng': 'orang', 'kmrn': 'kemarin', 'dmn': 'di mana', 'kpn': 'kapan', 'gmna': 'gimana',
    'bgmn': 'bagaimana', 'ok': 'oke', 'pls': 'tolong', 'btw': 'ngomong-ngomong', 'amp': 'dan',
    'cuma': 'hanya', 'emg': 'memang', 'emng': 'memang', 'gue': 'saya', 'gw': 'saya', 'gua': 'saya',
    'loe': 'kamu', 'lu': 'kamu', 'u': 'kamu', 'nih': 'ini', 'kek': 'kayak', 'jg': 'juga',
    'skrg': 'sekarang', 'uppppp': 'up', 'bikinin': 'bikin', 'lbh': 'lebih', 'd': 'di',
    'drpd': 'daripada', 'ayok': 'ayo', 'yuk': 'ayo', 'eleh': 'mengeluh', 'ats': 'atas',
    'sbg': 'sebagai', 'dpt': 'dapat', 'makasih': 'terima kasih', 'thx': 'terima kasih',
    'thanks': 'terima kasih', 'ntar': 'nanti', 'besok2': 'besok-besok', 'smg': 'semoga',
    'y': 'ya', 'bt': 'banget', 'gaes': 'teman-teman', 'temen': 'teman',
    'bolong2nya': 'bolong-bolongnya', 'bolong2': 'bolong-bolong', 'anak2': 'anak-anak', 'mbg':'makan bergizi gratis'
}

# Fungsi untuk mengganti kata slang di teks dengan kata baku berdasarkan kamus
def normalize_text_with_extended_dict(text):
    words = text.lower().split()  # ubah ke lowercase dan pisah kata
    normalized_words = [extended_normalization_dict.get(word, word) for word in words]  # ganti kata
    return ' '.join(normalized_words)  # gabungkan kata kembali

# Terapkan normalisasi ke kolom teks dan simpan hasilnya ke kolom baru
df['normalized_text'] = df['lower_text'].apply(normalize_text_with_extended_dict)

# Tampilkan contoh hasil normalisasi
print(df[['lower_text', 'normalized_text']].head(10))

"""Stopword Removal"""

# Daftar stopword khusus
custom_words = [
    #–– kata dasar umum
    'aku','anda','adalah','atau','akan','bagi','bahwa','bila','begitu','bersama',
    'bisa','buat','bukan','dalam','dan','dengan','dari','di','dia','jika','juga',
    'jadi','karena','kami','kita','ke','lagi','lalu','maka','masih','mau',
    'memang','menjadi','mengapa','mereka','meskipun','mesti','mungkin','pada',
    'pasti','saat','saya','sangat','saja','seperti','semua','sejak','selalu',
    'sedang','setelah','sementara','sudah','supaya','tapi','tentu','tentang',
    'terhadap','terus','tiap','waktu','yang','yaitu','kamu', 'ini', 'itu', 'ada',
    'untuk', 'kalau', 'dulu', 'mana', 'apa', 'sama','lihat', 'biar', 'pak',
    'presiden',

    #–– gaul / seruan / filler
    'ya','nah','kan','dong','nih','gak','nggak','ga','aja','kok','doang','udah',
    'banget','emang','gitu','padahal','malah','loh','weh','cie','ciee','cieee',
    'ok','oke','yah','yahh','yahhh','plis','please','hadeh','hadeuh','ampun',

    #–– tawa & umpatan
    'hah','haha','hahaha','hehe','hehehe','hihi','hihihi','wkwk','wkwkwk','anjir',
    'anjrit','anjr','anj','njir','coy','cok','jir','bjir',

    #–– partikel kecil
    'pas','kah','sih','deh','oh','ih','lah','tuh','si','mu','pa','ata','jela', 'nya'
]

# Hilangkan duplikat & jadikan set
stopwords_custom = set(custom_words)

# Fungsi untuk menghapus kata stopword dari teks
def remove_custom_stopwords(text: str) -> str:
    return ' '.join([w for w in text.split() if w not in stopwords_custom])

# Terapkan fungsi stopword removal pada kolom 'normalized_text', simpan ke kolom baru
df['stopwords_text'] = df['normalized_text'].apply(remove_custom_stopwords)

# Tampilkan contoh hasil setelah stopword removal
print(df[['normalized_text', 'stopwords_text']].head(10))

"""Tokenized"""

# Tokenisasi teks: pecah kalimat di kolom 'stopwords_text' jadi list kata
tokenized = df['stopwords_text'].apply(lambda x: x.split())

# Simpan hasil tokenisasi ke kolom baru 'tokenized_text'
df['tokenized_text'] = df['stopwords_text'].apply(lambda x: x.split())

# Tampilkan 10 baris pertama untuk melihat hasil tokenisasi
df.head(10)

"""Stemming"""

# Import stemmer dari pustaka Sastrawi untuk stemming bahasa Indonesia
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

# Buat objek stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Terapkan stemming pada setiap kata di kolom 'tokenized_text'
df['stemmed_text'] = df['tokenized_text'].apply(lambda x: [stemmer.stem(word) for word in x])

# Tampilkan 5 baris pertama hasil stemming
df.head(5)

df.to_csv('stemmed_dataset_yt.csv', index=False)

# prompt: connet to drive

from google.colab import drive
drive.mount('/content/drive')

"""# **LABELING**"""

# Membuat Teks Ditampilkan Full
pd.set_option('display.max_colwidth', None)

# Membaca file CSV hasil stemming dari Google Drive ke DataFrame
df = pd.read_csv('/content/drive/MyDrive/Skripsi_Dimas Aryo Wibowo_21.11.3972/Dataset/stemmed_dataset_yt.csv')

# Menampilkan 10 baris pertama DataFrame untuk melihat isi data
df.head(10)

# Salin isi kolom 'stemmed_text' ke kolom baru bernama 'text'
df['text'] = df['stemmed_text']

# Tampilkan 5 baris pertama untuk verifikasi
df.head()

for i, text in enumerate(df['text']):
    # Hapus karakter tanda kutip, koma, kurung siku dari string di baris i
    df['text'][i] = df['text'][i].replace("'", "").replace(',', '')\
                                 .replace(']', '').replace('[', '')

    list_words = []
    # Pisahkan string menjadi kata-kata dan simpan dalam list
    for word in df['text'][i].split():
        list_words.append(word)

    # Simpan kembali list kata-kata ke kolom 'text' di baris i
    df['text'][i] = list_words

# Tampilkan 5 baris pertama untuk cek hasil
df.head()

import pandas as pd
import csv

# Membuat kamus (dictionary) untuk lexicon kata positif
lexicon_positive = dict()
with open('/content/drive/MyDrive/Skripsi_Dimas Aryo Wibowo_21.11.3972/Dataset/lexicon_positive.csv', 'r') as csvfile:
    reader = csv.reader(csvfile, delimiter=';')
    for row in reader:
        lexicon_positive[row[0]] = int(row[1])

# Membuat kamus (dictionary) untuk lexicon kata negatif
lexicon_negative = dict()
with open('/content/drive/MyDrive/Skripsi_Dimas Aryo Wibowo_21.11.3972/Dataset/lexicon_negative.csv', 'r') as csvfile:
    reader = csv.reader(csvfile, delimiter=';')
    for row in reader:
        lexicon_negative[row[0]] = int(row[1])

# Fungsi analisis sentimen berbasis lexicon bahasa Indonesia
def sentiment_analysis_lexicon_indonesia(text):
    score = 0
    # Hitung skor positif berdasarkan kata dalam teks
    for word in text:
        if (word in lexicon_positive):
            score += lexicon_positive[word]
    # Hitung skor negatif berdasarkan kata dalam teks
    for word in text:
        if (word in lexicon_negative):
            score += lexicon_negative[word]

    # Tentukan polaritas atau drop jika netral
    if (score > 0):
        polarity = 'positif'
        return score, polarity
    elif (score < 0):
        polarity = 'negatif'
        return score, polarity
    else:
        return None  # Data netral akan di-drop

# Terapkan fungsi analisis sentimen ke tiap baris teks, hasil berupa tuple (score, polarity)
results = df['text'].apply(sentiment_analysis_lexicon_indonesia)

# Filter hasil yang tidak None (buang score = 0)
results = results.dropna()
df = df.loc[results.index].copy()

# Pecah hasil (score, polarity) ke dua kolom baru
df[['polarity_score', 'polarity']] = pd.DataFrame(results.tolist(), index=results.index)


# Tampilkan jumlah data untuk masing-masing kelas polaritas
print(df['polarity'].value_counts())

# Tampilkan 5 baris pertama dataframe untuk cek hasil
df.head()

# Konversi label polaritas dari string ('positif'/'negatif') ke numerik (1/0)
df['polarity'] = df['polarity'].apply(lambda x: 1 if x == 'positif' else 0)

# Tampilkan 5 baris pertama untuk verifikasi perubahan
df.head()

df.to_csv('labelingdataset_yt.csv', index=False)

"""# **EDA**"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Skripsi_Dimas Aryo Wibowo_21.11.3972/Dataset/labelingdataset_yt.csv')

# Impor WordCloud untuk visualisasi
from wordcloud import WordCloud

# Gabungkan semua kata dari kolom 'text' menjadi satu string panjang
list_words = ''
for tweet in df['text']:
    for word in tweet:
        list_words += ' ' + word

# Buat objek WordCloud dengan ukuran dan pengaturan dasar
wordcloud = WordCloud(width=600, height=400,
                      background_color='white', min_font_size=10).generate(list_words)

# Buat figure dan axis untuk menampilkan WordCloud
fig, ax = plt.subplots(figsize=(8, 6))

# Atur judul visualisasi
ax.set_title('Word Cloud Dataset Program Makan Bergizi Gratis', fontsize=18)

# Nonaktifkan grid
ax.grid(False)

# Tampilkan word cloud di axis
ax.imshow(wordcloud)

# Atur tata letak agar pas
fig.tight_layout(pad=0)

# Sembunyikan sumbu
ax.axis('off')

# Tampilkan visualisasi
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Gunakan gaya seaborn agar lebih menarik
plt.style.use("seaborn-v0_8-darkgrid")

# Pilih palet warna yang lebih smooth
colors = sns.color_palette("coolwarm_r", n_colors=2)

# Menghitung jumlah dan persentase
sizes = df['polarity'].value_counts().values
labels = df['polarity'].value_counts().index
total = sum(sizes)  # Total jumlah tweets
explode = (0.05, 0.05)  # Efek memisahkan slice sedikit

# Format label dengan jumlah dan persentase
def autopct_format(pct, all_vals):
    absolute = int(round(pct * sum(all_vals) / 100.0))  # Konversi ke jumlah asli
    return f"{absolute} data\n({pct:.1f}%)"  # Format jumlah dan persentase

# Membuat subplot lebih lebar agar proporsional
fig, ax = plt.subplots(figsize=(7, 7))

# Pie chart dengan efek lebih modern
wedges, texts, autotexts = ax.pie(
    x=sizes,
    labels=labels,
    colors=colors,
    autopct=lambda pct: autopct_format(pct, sizes),  # Format label
    explode=explode,
    textprops={'fontsize': 14, 'weight': 'bold'},  # Font lebih jelas
    wedgeprops={'linewidth': 1.5, 'edgecolor': 'white'},  # Outline putih agar kontras
    shadow=True,  # Efek bayangan 3D
    startangle=140,  # Sudut awal agar terlihat lebih estetik
    pctdistance=0.8  # Memindahkan persentase agar tidak menumpuk
)

# Perbaiki warna teks agar lebih sesuai dengan background
for text in texts:
    text.set_color("black")

for autotext in autotexts:
    autotext.set_color("black")  # Persentase dibuat lebih kontras

# Tambahkan judul lebih menarik
ax.set_title(f'Sentiment Label\n(total = {total} Data)', fontsize=18, pad=20, weight='bold')

# Tampilkan pie chart
plt.show()

# Hitung panjang setiap tweet
df['tweet_length'] = df['stemmed_text'].astype(str).apply(len)

# Tampilkan 5 baris pertama DataFrame setelah diproses
df.head()

# Panjang berdasarkan karakter
length_karakter = df['tweet_length']

# Histogram Panjang Karakter
plt.figure(figsize=(8, 4))
plt.hist(length_karakter, bins=30, alpha=0.7, color='steelblue', edgecolor='black')
plt.axvline(x=100, color='red', linestyle='--', linewidth=1.5, label='max_len = 100')
plt.title('Distribusi Panjang Tweet Berdasarkan Karakter', fontsize=14)
plt.xlabel('Jumlah Karakter', fontsize=12)
plt.ylabel('Frekuensi', fontsize=12)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Statistik Tambahan
print("=== Statistik Panjang Tweet ===")
print("Berdasarkan Karakter (tweet_length):")
print("Rata-rata :", round(length_karakter.mean(), 2))
print("Median    :", length_karakter.median())
print("Maksimum  :", length_karakter.max())

# Panjang berdasarkan token
length_token = df['text'].apply(len)

# Distribusi panjang token
token_counts = length_token.value_counts().sort_index()

# Line Plot Panjang Token
plt.figure(figsize=(8, 4))
plt.plot(token_counts.index, token_counts.values, marker='o', color='darkorange', linewidth=2.5, label='Jumlah Token')
plt.fill_between(token_counts.index, token_counts.values, color='orange', alpha=0.3)
plt.axvline(x=100, color='purple', linestyle='--', linewidth=1.5, label='max token = 100')
plt.title('Distribusi Panjang Tweet Berdasarkan Token', fontsize=14)
plt.xlabel('Jumlah Token', fontsize=12)
plt.ylabel('Frekuensi', fontsize=12)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Statistik Tambahan
print("=== Statistik Panjang Tweet ===")
print("Berdasarkan Token (stemmed):")
print("Rata-rata :", round(length_token.mean(), 2))
print("Median    :", length_token.median())
print("Maksimum  :", length_token.max())

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Memisahkan data berdasarkan label
positive_data = df[df['polarity'] == 1]['stemmed_text']
negative_data = df[df['polarity'] == 0]['stemmed_text']

# Menggabungkan semua teks untuk masing-masing label menjadi satu string
positive_text = ' '.join(positive_data.dropna().apply(lambda x: ' '.join(eval(x))).tolist())
negative_text = ' '.join(negative_data.dropna().apply(lambda x: ' '.join(eval(x))).tolist())

# Membuat WordCloud untuk ulasan positif
positive_wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Greens').generate(positive_text)

# Membuat WordCloud untuk ulasan negatif
negative_wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate(negative_text)

# Menampilkan WordCloud
plt.figure(figsize=(16, 8))

# WordCloud positif
plt.subplot(1, 2, 1)
plt.imshow(positive_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud Positif', fontsize=20)

# WordCloud negatif
plt.subplot(1, 2, 2)
plt.imshow(negative_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud Negatif', fontsize=20)

plt.tight_layout()
plt.show()

"""# **PREMODELING**"""

# Membaca dataset berlabel dari file CSV di Google Drive ke dalam DataFrame
df = pd.read_csv('/content/drive/MyDrive/Skripsi_Dimas Aryo Wibowo_21.11.3972/Dataset/labelingdataset_yt.csv')

# Menampilkan 5 baris pertama untuk melihat isi dataset
df.head(5)

# Hapus kolom 'text' dari DataFrame secara permanen
df.drop(columns=['text'], inplace=True)

# Tampilkan 5 baris pertama untuk verifikasi kolom sudah dihapus
df.head()

import ast

# Mengubah string list di 'stemmed_text' menjadi list sebenarnya lalu gabungkan jadi kalimat string
x = df['stemmed_text'].apply(lambda x: ' '.join(ast.literal_eval(x)))

# Target label polaritas
y = df['polarity']

# Membagi data jadi train (80%) dan test (20%), dengan stratifikasi agar distribusi label tetap seimbang
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.20, random_state=42, stratify=y
)

df_plot = pd.DataFrame({
    'text': list(x_train) + list(x_test),
    'dataset': ['Train'] * len(x_train) + ['Test'] * len(x_test)
})

# Plotting
plt.figure(figsize=(8, 6))
sns.countplot(x='dataset', data=df_plot, palette='coolwarm')
plt.title('Split Data Train & Test')
plt.xlabel('Dataset')
plt.ylabel('Jumlah')
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Hitung distribusi label
train_counts = y_train.value_counts().sort_index()
test_counts = y_test.value_counts().sort_index()

# Tampilkan jumlah masing-masing label
print("Distribusi Label pada Data Train:")
print(train_counts)
print("\nDistribusi Label pada Data Test:")
print(test_counts)
print()

# Warna soft khusus label
soft_colors = ['#FFCCCB', '#AEC6CF']  # 0 = merah muda, 1 = biru muda

# Plot Train
plt.figure(figsize=(6, 4))
sns.barplot(x=train_counts.index, y=train_counts.values, palette=soft_colors)
plt.title('Distribusi Label pada Data Train')
plt.xlabel('Label')
plt.ylabel('Jumlah')
plt.xticks([0, 1], ['Kontra (0)', 'Pro (1)'])
plt.grid(axis='y')
plt.tight_layout()
plt.show()
print()

# Plot Test
plt.figure(figsize=(6, 4))
sns.barplot(x=test_counts.index, y=test_counts.values, palette=soft_colors)
plt.title('Distribusi Label pada Data Test')
plt.xlabel('Label')
plt.ylabel('Jumlah')
plt.xticks([0, 1], ['Kontra (0)', 'Pro (1)'])
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# Import modul dari TensorFlow Keras untuk preprocessing teks dan pembuatan model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Membuat tokenizer dengan batasan 3000 kata terbanyak dan mengubah semua teks jadi huruf kecil
tokenizer = Tokenizer(num_words=3000, lower=True)
tokenizer.fit_on_texts(x_train)  # Pelajari kata-kata dari data latih

# Ubah teks latih dan tes menjadi urutan indeks kata (tokenisasi)
x_train_seq = tokenizer.texts_to_sequences(x_train)
x_test_seq = tokenizer.texts_to_sequences(x_test)

# Padding sequence agar panjangnya sama, maksimal 50 kata per teks
x_train_pad = pad_sequences(x_train_seq, maxlen=100)
x_test_pad = pad_sequences(x_test_seq, maxlen=100)

from ast import literal_eval
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Ubah string list ke list asli, lalu gabungkan jadi kalimat
texts = df['stemmed_text'].apply(literal_eval).apply(lambda x: ' '.join(x))

# Inisialisasi tokenizer
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(texts)

# Tokenisasi dan padding
sequences = tokenizer.texts_to_sequences(texts)
max_length = 100
padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')

# Tampilkan hanya 5 data teratas
for i, (text, sequence, padded_sequence) in enumerate(zip(texts, sequences, padded_sequences)):
    if i == 5:
        break
    print(f"Original Text {i+1}: {text}")
    print(f"Tokenized Sequence {i+1}: {sequence}")
    print(f"Padded Sequence {i+1}: {padded_sequence}")
    print("-" * 50)

import pickle

# Simpan Tokenizer ke file
with open('tokenizeryt.pkl', 'wb') as file:
    pickle.dump(tokenizer, file)

from imblearn.over_sampling import SMOTE
from collections import Counter

# Cek distribusi data sebelum SMOTE
print("Distribusi sebelum SMOTE:")
print(Counter(y_train))

# Terapkan SMOTE
smote = SMOTE(random_state=42)
x_train_smote, y_train_smote = smote.fit_resample(x_train_pad, y_train)

# Cek distribusi data setelah SMOTE
print("\nDistribusi setelah SMOTE:")
print(Counter(y_train_smote))

# Menampilkan shape setelah SMOTE
print("\nShape of x_train_smote:", x_train_smote.shape)
print("Shape of y_train_smote:", y_train_smote.shape)

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Sebelum SMOTE
before_smote = pd.Series(y_train, name='Label')

# Plot distribusi sebelum SMOTE
plt.figure(figsize=(8, 8))
sns.countplot(x=before_smote, palette="coolwarm")
plt.title("Distribusi Sebelum SMOTE")
plt.xlabel("Sentimen")
plt.ylabel("Jumlah")
plt.show()

# Setelah SMOTE
after_smote = pd.Series(y_train_smote, name='Label')

# Plot distribusi setelah SMOTE
plt.figure(figsize=(8, 8))
sns.countplot(x=after_smote, palette="coolwarm")
plt.title("Distribusi Setelah SMOTE")
plt.xlabel("Sentimen")
plt.ylabel("Jumlah")
plt.show()

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding

# Parameter embedding
embedding_dim = 100
max_len = 100
max_words = 10000

# Buat model dengan layer embedding
embedding_model = Sequential()
embedding_model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))

# Dapatkan output embedding dari semua padded_sequences
embedding_output = embedding_model.predict(padded_sequences)

# Tampilkan hanya 5 data teratas
for i, (text, padded, embedding) in enumerate(zip(texts, padded_sequences, embedding_output)):
    if i == 5:
        break
    print(f"\nTeks Asli {i+1}: {text}")
    print(f"Encoding (padded): {padded}")
    print(f"Embedding Output {i+1} (shape: {embedding.shape}):")
    print(embedding)  # Array (60, 100): 60 token, 100 dimensi
    print("=" * 100)

# Import Library
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# ========================================
# Definisi Model LSTM
# ========================================

# Input Layer
inputs = Input(shape=(max_len,))

# Embedding Layer
embedding = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len)(inputs)

# LSTM
lstm = LSTM(64, return_sequences=False)(embedding)  # Tidak mengembalikan urutan karena pooling langsung

# Dropout untuk mencegah overfitting
dropout = Dropout(0.5)(lstm)

# Dense Layer (Output)
output = Dense(1, activation='sigmoid')(dropout)

# Membuat Model
model = Model(inputs=inputs, outputs=output)

# Compile Model
model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

# Summary Model
model.summary()

# ========================================
# Pelatihan Model
# ========================================
# Callback EarlyStopping
early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True, mode='max', verbose=1)

# Latih Model
history = model.fit(
    x_train_smote, y_train_smote,  # Data latih dan label
    epochs=10,
    batch_size=32,
    validation_data=(x_test_pad, y_test),  # Data validasi
    callbacks=[early_stopping]  # Early stopping
)

# ========================================
# Evaluasi Model
# ========================================
loss, accuracy = model.evaluate(x_test_pad, y_test)
print(f'Loss: {loss}, Accuracy: {accuracy}')

# ========================================
# Visualisasi Hasil Pelatihan
# ========================================
plt.figure(figsize=(12, 4))

# Plot Akurasi
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# Import libraries
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Memprediksi label untuk data uji
y_test_pred = model.predict(x_test_pad)
y_test_pred_classes = (y_test_pred > 0.5).astype(int).reshape(-1)  # Ambil prediksi sebagai 0 atau 1

# Menghasilkan matriks kebingungan
conf_matrix = confusion_matrix(y_test, y_test_pred_classes)

# Membuat plot matriks kebingungan
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.xlabel('True Label')
plt.ylabel('Predicted Label')
plt.title('Confusion Matrix - LSTM')
plt.show()

# Mencetak laporan klasifikasi
print("Classification Report:")
print(classification_report(y_test, y_test_pred_classes, target_names=['Negative', 'Positive']))

# Setelah model selesai dilatih
model.save("modelyt.h5")